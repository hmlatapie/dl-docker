{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression in Theano\n",
    "\n",
    "Credits: Forked from [summerschool2015](https://github.com/mila-udem/summerschool2015) by mila-udem\n",
    "\n",
    "This notebook is inspired from [the tutorial on logistic regression](http://deeplearning.net/tutorial/logreg.html) on [deeplearning.net](http://deeplearning.net).\n",
    "\n",
    "In this notebook, we show how Theano can be used to implement the most basic classifier: the **logistic regression**. We start off with a quick primer of the model, which serves both as a refresher but also to anchor the notation and show how mathematical expressions are mapped onto Theano graphs.\n",
    "\n",
    "In the deepest of machine learning traditions, this tutorial will tackle the exciting problem of MNIST digit classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "In the mean time, let's just download a pre-packaged version of MNIST, and load each split of the dataset as NumPy ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import six\n",
    "from six.moves import cPickle\n",
    "\n",
    "if not os.path.exists('mnist.pkl.gz'):\n",
    "    r = requests.get('http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz')\n",
    "    with open('mnist.pkl.gz', 'wb') as data_file:\n",
    "        data_file.write(r.content)\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as data_file:\n",
    "    if six.PY3:\n",
    "        train_set, valid_set, test_set = cPickle.load(data_file, encoding='latin1')\n",
    "    else:\n",
    "        train_set, valid_set, test_set = cPickle.load(data_file)\n",
    "\n",
    "train_set_x, train_set_y = train_set\n",
    "valid_set_x, valid_set_y = valid_set\n",
    "test_set_x, test_set_y = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized\n",
    "by a weight matrix $W$ and a bias vector $b$. Classification is\n",
    "done by projecting an input vector onto a set of hyperplanes, each of which\n",
    "corresponds to a class. The distance from the input to a hyperplane reflects\n",
    "the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $x$ is a member of a\n",
    "class $i$, a value of a stochastic variable $Y$, can be written as:\n",
    "\n",
    "$$P(Y=i|x, W,b) = softmax_i(W x + b) = \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}$$\n",
    "\n",
    "The model's prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "\n",
    "$$  y_{pred} = {\\rm argmax}_i P(Y=i|x,W,b)$$\n",
    "\n",
    "Now, let us define our input variables. First, we need to define the dimension of our tensors:\n",
    "- `n_in` is the length of each training vector,\n",
    "- `n_out` is the number of classes.\n",
    "\n",
    "Our variables will be:\n",
    "- `x` is a matrix, where each row contains a different example of the dataset. Its shape is `(batch_size, n_in)`, but `batch_size` does not have to be specified in advance, and can change during training.\n",
    "- `W` is a shared matrix, of shape `(n_in, n_out)`, initialized with zeros. Column `k` of `W` represents the separation hyperplane for class `k`.\n",
    "- `b` is a shared vector, of length `n_out`, initialized with zeros. Element `k` of `b` represents the free parameter of hyperplane `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import theano\n",
    "from theano import tensor\n",
    "\n",
    "# Size of the data\n",
    "n_in = 28 * 28\n",
    "# Number of classes\n",
    "n_out = 10\n",
    "\n",
    "x = tensor.matrix('x')\n",
    "W = theano.shared(value=numpy.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
    "                  name='W',\n",
    "                  borrow=True)\n",
    "b = theano.shared(value=numpy.zeros((n_out,), dtype=theano.config.floatX),\n",
    "                  name='b',\n",
    "                  borrow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build a symbolic expression for the matrix of class-membership probability (`p_y_given_x`), and for the class whose probability is maximal (`y_pred`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_y_given_x = tensor.nnet.softmax(tensor.dot(x, W) + b)\n",
    "y_pred = tensor.argmax(p_y_given_x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a loss function\n",
    "Learning optimal model parameters involves minimizing a loss function. In the\n",
    "case of multi-class logistic regression, it is very common to use the negative\n",
    "log-likelihood as the loss. This is equivalent to maximizing the likelihood of the\n",
    "data set $\\cal{D}$ under the model parameterized by $\\theta$. Let\n",
    "us first start by defining the likelihood $\\cal{L}$ and loss\n",
    "$\\ell$:\n",
    "\n",
    "$$\\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "     \\sum_{i=0}^{|\\mathcal{D}|} \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "   \\ell (\\theta=\\{W,b\\}, \\mathcal{D}) = - \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "$$\n",
    "\n",
    "Again, we will express those expressions using Theano. We have one additional input, the actual target class `y`:\n",
    "- `y` is an input vector of integers, of length `batch_size` (which will have to match the length of `x` at runtime). The length of `y` can be symbolically expressed by `y.shape[0]`.\n",
    "- `log_prob` is a `(batch_size, n_out)` matrix containing the log probabilities of class membership for each example.\n",
    "- `arange(y.shape[0])` is a symbolic vector which will contain `[0,1,2,... batch_size-1]`\n",
    "- `log_likelihood` is a vector containing the log probability of the target, for each example.\n",
    "- `loss` is the mean of the negative `log_likelihood` over the examples in the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tensor.lvector('y')\n",
    "log_prob = tensor.log(p_y_given_x)\n",
    "log_likelihood = log_prob[tensor.arange(y.shape[0]), y]\n",
    "loss = - log_likelihood.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure\n",
    "This notebook will use the method of stochastic gradient descent with mini-batches (MSGD) to find values of `W` and `b` that minimize the loss.\n",
    "\n",
    "We can let Theano compute symbolic expressions for the gradient of the loss wrt `W` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_W, g_b = theano.grad(cost=loss, wrt=[W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g_W` and `g_b` are symbolic variables, which can be used as part of a computation graph. In particular, let us define the expressions for one step of gradient descent for `W` and `b`, for a fixed learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = numpy.float32(0.13)\n",
    "new_W = W - learning_rate * g_W\n",
    "new_b = b - learning_rate * g_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define **update expressions**, or pairs of (shared variable, expression for its update), that we will use when compiling the Theano function. The updates will be performed each time the function gets called.\n",
    "\n",
    "The following function, `train_model`, returns the loss on the current minibatch, then changes the values of the shared variables according to the update rules. It needs to be passed `x` and `y` as inputs, but not the shared variables, which are implicit inputs.\n",
    "\n",
    "The entire learning algorithm thus consists in looping over all examples in the dataset, considering all the examples in one minibatch at a time, and repeatedly calling the `train_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model = theano.function(inputs=[x, y],\n",
    "                              outputs=loss,\n",
    "                              updates=[(W, new_W),\n",
    "                                       (b, new_b)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "When testing the model, we are interested in the number of misclassified examples (and not only in the likelihood). Here, we build a symbolic expression for retrieving the number of misclassified examples in a minibatch.\n",
    "\n",
    "This will also be useful to apply on the validation and testing sets, in order to monitor the progress of the model during training, and to do early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "misclass_nb = tensor.neq(y_pred, y)\n",
    "misclass_rate = misclass_nb.mean()\n",
    "\n",
    "test_model = theano.function(inputs=[x, y],\n",
    "                             outputs=misclass_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Here is the main training loop of the algorithm:\n",
    "- For each *epoch*, or pass through the training set\n",
    "  - split the training set in minibatches, and call `train_model` on each minibatch\n",
    "  - split the validation set in minibatches, and call `test_model` on each minibatch to measure the misclassification rate\n",
    "  - if the misclassification rate has not improved in a while, stop training\n",
    "- Measure performance on the test set\n",
    "\n",
    "The **early stopping procedure** is what decide whether the performance has improved enough. There are many variants, and we will not go into the details of this one here.\n",
    "\n",
    "We first need to define a few parameters for the training loop and the early stopping procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define a couple of helper variables and functions for the optimization\n",
    "batch_size = 500\n",
    "# compute number of minibatches for training, validation and testing\n",
    "n_train_batches = train_set_x.shape[0] // batch_size\n",
    "n_valid_batches = valid_set_x.shape[0] // batch_size\n",
    "n_test_batches = test_set_x.shape[0] // batch_size\n",
    "\n",
    "def get_minibatch(i, dataset_x, dataset_y):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "    batch_x = dataset_x[start_idx:end_idx]\n",
    "    batch_y = dataset_y[start_idx:end_idx]\n",
    "    return (batch_x, batch_y)\n",
    "\n",
    "## early-stopping parameters\n",
    "# maximum number of epochs\n",
    "n_epochs = 1000\n",
    "# look as this many examples regardless\n",
    "patience = 5000\n",
    "# wait this much longer when a new best is found\n",
    "patience_increase = 2\n",
    "# a relative improvement of this much is considered significant\n",
    "improvement_threshold = 0.995\n",
    "\n",
    "# go through this many minibatches before checking the network on the validation set;\n",
    "# in this case we check every epoch\n",
    "validation_frequency = min(n_train_batches, patience / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch 100/100, validation error 11.890000 %\n",
      "     epoch 1, minibatch 100/100, test error of  best model 12.170000 %\n",
      "epoch 2, minibatch 100/100, validation error 10.520000 %\n",
      "     epoch 2, minibatch 100/100, test error of  best model 10.940000 %\n",
      "epoch 3, minibatch 100/100, validation error 9.940000 %\n",
      "     epoch 3, minibatch 100/100, test error of  best model 10.210000 %\n",
      "epoch 4, minibatch 100/100, validation error 9.480000 %\n",
      "     epoch 4, minibatch 100/100, test error of  best model 9.820000 %\n",
      "epoch 5, minibatch 100/100, validation error 9.230000 %\n",
      "     epoch 5, minibatch 100/100, test error of  best model 9.510000 %\n",
      "epoch 6, minibatch 100/100, validation error 9.100000 %\n",
      "     epoch 6, minibatch 100/100, test error of  best model 9.280000 %\n",
      "epoch 7, minibatch 100/100, validation error 8.820000 %\n",
      "     epoch 7, minibatch 100/100, test error of  best model 9.100000 %\n",
      "epoch 8, minibatch 100/100, validation error 8.750000 %\n",
      "     epoch 8, minibatch 100/100, test error of  best model 8.890000 %\n",
      "epoch 9, minibatch 100/100, validation error 8.630000 %\n",
      "     epoch 9, minibatch 100/100, test error of  best model 8.780000 %\n",
      "epoch 10, minibatch 100/100, validation error 8.490000 %\n",
      "     epoch 10, minibatch 100/100, test error of  best model 8.620000 %\n",
      "epoch 11, minibatch 100/100, validation error 8.440000 %\n",
      "     epoch 11, minibatch 100/100, test error of  best model 8.570000 %\n",
      "epoch 12, minibatch 100/100, validation error 8.380000 %\n",
      "     epoch 12, minibatch 100/100, test error of  best model 8.530000 %\n",
      "epoch 13, minibatch 100/100, validation error 8.310000 %\n",
      "     epoch 13, minibatch 100/100, test error of  best model 8.480000 %\n",
      "epoch 14, minibatch 100/100, validation error 8.250000 %\n",
      "     epoch 14, minibatch 100/100, test error of  best model 8.400000 %\n",
      "epoch 15, minibatch 100/100, validation error 8.270000 %\n",
      "epoch 16, minibatch 100/100, validation error 8.220000 %\n",
      "     epoch 16, minibatch 100/100, test error of  best model 8.300000 %\n",
      "epoch 17, minibatch 100/100, validation error 8.160000 %\n",
      "     epoch 17, minibatch 100/100, test error of  best model 8.210000 %\n",
      "epoch 18, minibatch 100/100, validation error 8.100000 %\n",
      "     epoch 18, minibatch 100/100, test error of  best model 8.200000 %\n",
      "epoch 19, minibatch 100/100, validation error 8.070000 %\n",
      "     epoch 19, minibatch 100/100, test error of  best model 8.190000 %\n",
      "epoch 20, minibatch 100/100, validation error 8.080000 %\n",
      "epoch 21, minibatch 100/100, validation error 8.030000 %\n",
      "     epoch 21, minibatch 100/100, test error of  best model 8.160000 %\n",
      "epoch 22, minibatch 100/100, validation error 8.020000 %\n",
      "     epoch 22, minibatch 100/100, test error of  best model 8.180000 %\n",
      "epoch 23, minibatch 100/100, validation error 7.980000 %\n",
      "     epoch 23, minibatch 100/100, test error of  best model 8.110000 %\n",
      "epoch 24, minibatch 100/100, validation error 7.970000 %\n",
      "     epoch 24, minibatch 100/100, test error of  best model 8.130000 %\n",
      "epoch 25, minibatch 100/100, validation error 7.950000 %\n",
      "     epoch 25, minibatch 100/100, test error of  best model 8.110000 %\n",
      "epoch 26, minibatch 100/100, validation error 7.940000 %\n",
      "     epoch 26, minibatch 100/100, test error of  best model 8.090000 %\n",
      "epoch 27, minibatch 100/100, validation error 7.890000 %\n",
      "     epoch 27, minibatch 100/100, test error of  best model 8.060000 %\n",
      "epoch 28, minibatch 100/100, validation error 7.880000 %\n",
      "     epoch 28, minibatch 100/100, test error of  best model 8.030000 %\n",
      "epoch 29, minibatch 100/100, validation error 7.830000 %\n",
      "     epoch 29, minibatch 100/100, test error of  best model 7.970000 %\n",
      "epoch 30, minibatch 100/100, validation error 7.800000 %\n",
      "     epoch 30, minibatch 100/100, test error of  best model 7.930000 %\n",
      "epoch 31, minibatch 100/100, validation error 7.780000 %\n",
      "     epoch 31, minibatch 100/100, test error of  best model 7.900000 %\n",
      "epoch 32, minibatch 100/100, validation error 7.760000 %\n",
      "     epoch 32, minibatch 100/100, test error of  best model 7.850000 %\n",
      "epoch 33, minibatch 100/100, validation error 7.750000 %\n",
      "     epoch 33, minibatch 100/100, test error of  best model 7.840000 %\n",
      "epoch 34, minibatch 100/100, validation error 7.760000 %\n",
      "epoch 35, minibatch 100/100, validation error 7.780000 %\n",
      "epoch 36, minibatch 100/100, validation error 7.750000 %\n",
      "epoch 37, minibatch 100/100, validation error 7.730000 %\n",
      "     epoch 37, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 38, minibatch 100/100, validation error 7.720000 %\n",
      "     epoch 38, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 39, minibatch 100/100, validation error 7.700000 %\n",
      "     epoch 39, minibatch 100/100, test error of  best model 7.780000 %\n",
      "epoch 40, minibatch 100/100, validation error 7.690000 %\n",
      "     epoch 40, minibatch 100/100, test error of  best model 7.800000 %\n",
      "epoch 41, minibatch 100/100, validation error 7.670000 %\n",
      "     epoch 41, minibatch 100/100, test error of  best model 7.790000 %\n",
      "epoch 42, minibatch 100/100, validation error 7.690000 %\n",
      "epoch 43, minibatch 100/100, validation error 7.690000 %\n",
      "epoch 44, minibatch 100/100, validation error 7.690000 %\n",
      "epoch 45, minibatch 100/100, validation error 7.680000 %\n",
      "epoch 46, minibatch 100/100, validation error 7.660000 %\n",
      "     epoch 46, minibatch 100/100, test error of  best model 7.820000 %\n",
      "epoch 47, minibatch 100/100, validation error 7.640000 %\n",
      "     epoch 47, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 48, minibatch 100/100, validation error 7.600000 %\n",
      "     epoch 48, minibatch 100/100, test error of  best model 7.790000 %\n",
      "epoch 49, minibatch 100/100, validation error 7.600000 %\n",
      "epoch 50, minibatch 100/100, validation error 7.590000 %\n",
      "     epoch 50, minibatch 100/100, test error of  best model 7.800000 %\n",
      "epoch 51, minibatch 100/100, validation error 7.600000 %\n",
      "epoch 52, minibatch 100/100, validation error 7.580000 %\n",
      "     epoch 52, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 53, minibatch 100/100, validation error 7.570000 %\n",
      "     epoch 53, minibatch 100/100, test error of  best model 7.820000 %\n",
      "epoch 54, minibatch 100/100, validation error 7.560000 %\n",
      "     epoch 54, minibatch 100/100, test error of  best model 7.820000 %\n",
      "epoch 55, minibatch 100/100, validation error 7.550000 %\n",
      "     epoch 55, minibatch 100/100, test error of  best model 7.820000 %\n",
      "epoch 56, minibatch 100/100, validation error 7.520000 %\n",
      "     epoch 56, minibatch 100/100, test error of  best model 7.830000 %\n",
      "epoch 57, minibatch 100/100, validation error 7.490000 %\n",
      "     epoch 57, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 58, minibatch 100/100, validation error 7.480000 %\n",
      "     epoch 58, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 59, minibatch 100/100, validation error 7.450000 %\n",
      "     epoch 59, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 60, minibatch 100/100, validation error 7.450000 %\n",
      "epoch 61, minibatch 100/100, validation error 7.430000 %\n",
      "     epoch 61, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 62, minibatch 100/100, validation error 7.430000 %\n",
      "epoch 63, minibatch 100/100, validation error 7.410000 %\n",
      "     epoch 63, minibatch 100/100, test error of  best model 7.810000 %\n",
      "epoch 64, minibatch 100/100, validation error 7.400000 %\n",
      "     epoch 64, minibatch 100/100, test error of  best model 7.790000 %\n",
      "epoch 65, minibatch 100/100, validation error 7.410000 %\n",
      "epoch 66, minibatch 100/100, validation error 7.410000 %\n",
      "epoch 67, minibatch 100/100, validation error 7.390000 %\n",
      "     epoch 67, minibatch 100/100, test error of  best model 7.770000 %\n",
      "epoch 68, minibatch 100/100, validation error 7.380000 %\n",
      "     epoch 68, minibatch 100/100, test error of  best model 7.770000 %\n",
      "epoch 69, minibatch 100/100, validation error 7.390000 %\n",
      "epoch 70, minibatch 100/100, validation error 7.370000 %\n",
      "     epoch 70, minibatch 100/100, test error of  best model 7.750000 %\n",
      "epoch 71, minibatch 100/100, validation error 7.370000 %\n",
      "epoch 72, minibatch 100/100, validation error 7.380000 %\n",
      "epoch 73, minibatch 100/100, validation error 7.380000 %\n",
      "epoch 74, minibatch 100/100, validation error 7.380000 %\n",
      "epoch 75, minibatch 100/100, validation error 7.350000 %\n",
      "     epoch 75, minibatch 100/100, test error of  best model 7.780000 %\n",
      "epoch 76, minibatch 100/100, validation error 7.330000 %\n",
      "     epoch 76, minibatch 100/100, test error of  best model 7.800000 %\n",
      "epoch 77, minibatch 100/100, validation error 7.320000 %\n",
      "     epoch 77, minibatch 100/100, test error of  best model 7.780000 %\n",
      "epoch 78, minibatch 100/100, validation error 7.310000 %\n",
      "     epoch 78, minibatch 100/100, test error of  best model 7.780000 %\n",
      "epoch 79, minibatch 100/100, validation error 7.310000 %\n",
      "epoch 80, minibatch 100/100, validation error 7.330000 %\n",
      "epoch 81, minibatch 100/100, validation error 7.300000 %\n",
      "     epoch 81, minibatch 100/100, test error of  best model 7.770000 %\n",
      "epoch 82, minibatch 100/100, validation error 7.290000 %\n",
      "     epoch 82, minibatch 100/100, test error of  best model 7.750000 %\n",
      "epoch 83, minibatch 100/100, validation error 7.290000 %\n",
      "epoch 84, minibatch 100/100, validation error 7.290000 %\n",
      "epoch 85, minibatch 100/100, validation error 7.300000 %\n",
      "epoch 86, minibatch 100/100, validation error 7.280000 %\n",
      "     epoch 86, minibatch 100/100, test error of  best model 7.710000 %\n",
      "epoch 87, minibatch 100/100, validation error 7.280000 %\n",
      "epoch 88, minibatch 100/100, validation error 7.260000 %\n",
      "     epoch 88, minibatch 100/100, test error of  best model 7.690000 %\n",
      "epoch 89, minibatch 100/100, validation error 7.260000 %\n",
      "epoch 90, minibatch 100/100, validation error 7.250000 %\n",
      "     epoch 90, minibatch 100/100, test error of  best model 7.700000 %\n",
      "epoch 91, minibatch 100/100, validation error 7.250000 %\n",
      "epoch 92, minibatch 100/100, validation error 7.230000 %\n",
      "     epoch 92, minibatch 100/100, test error of  best model 7.680000 %\n",
      "epoch 93, minibatch 100/100, validation error 7.230000 %\n",
      "epoch 94, minibatch 100/100, validation error 7.230000 %\n",
      "epoch 95, minibatch 100/100, validation error 7.220000 %\n",
      "     epoch 95, minibatch 100/100, test error of  best model 7.690000 %\n",
      "Optimization complete with best validation score of 7.220000 %, with test performance 7.690000 %\n",
      "The code ran for 96 epochs, with 14.540282 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from six.moves import xrange\n",
    "\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in xrange(n_train_batches):\n",
    "        minibatch_x, minibatch_y = get_minibatch(minibatch_index, train_set_x, train_set_y)\n",
    "        minibatch_avg_cost = train_model(minibatch_x, minibatch_y)\n",
    "\n",
    "        # iteration number\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            # compute zero-one loss on validation set\n",
    "            validation_losses = []\n",
    "            for i in xrange(n_valid_batches):\n",
    "                valid_xi, valid_yi = get_minibatch(i, valid_set_x, valid_set_y)\n",
    "                validation_losses.append(test_model(valid_xi, valid_yi))\n",
    "            this_validation_loss = numpy.mean(validation_losses)\n",
    "            print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                  (epoch,\n",
    "                   minibatch_index + 1,\n",
    "                   n_train_batches,\n",
    "                   this_validation_loss * 100.))\n",
    "\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                # improve patience if loss improvement is good enough\n",
    "                if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                best_validation_loss = this_validation_loss\n",
    "\n",
    "                # test it on the test set\n",
    "                test_losses = []\n",
    "                for i in xrange(n_test_batches):\n",
    "                    test_xi, test_yi = get_minibatch(i, test_set_x, test_set_y)\n",
    "                    test_losses.append(test_model(test_xi, test_yi))\n",
    "\n",
    "                test_score = numpy.mean(test_losses)\n",
    "                print('     epoch %i, minibatch %i/%i, test error of  best model %f %%' %\n",
    "                      (epoch,\n",
    "                       minibatch_index + 1,\n",
    "                       n_train_batches,\n",
    "                       test_score * 100.))\n",
    "\n",
    "                # save the best parameters\n",
    "                numpy.savez('best_model.npz', W=W.get_value(), b=b.get_value())\n",
    "\n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print('Optimization complete with best validation score of %f %%, '\n",
    "      'with test performance %f %%' %\n",
    "      (best_validation_loss * 100., test_score * 100.))\n",
    "\n",
    "print('The code ran for %d epochs, with %f epochs/sec' %\n",
    "      (epoch, 1. * epoch / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "You can visualize the columns of `W`, which correspond to the separation hyperplanes for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 143.5, 56.5, -0.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2IAAAFpCAYAAAD+95t4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwP8D3Tc7OKpMpMUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31d808b1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import tile_raster_images\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Increase the size of the figure\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "\n",
    "plot_data = tile_raster_images(W.get_value(borrow=True).T,\n",
    "                               img_shape=(28, 28), tile_shape=(2, 5), tile_spacing=(1, 1))\n",
    "plt.imshow(plot_data, cmap='Greys', interpolation='none')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
